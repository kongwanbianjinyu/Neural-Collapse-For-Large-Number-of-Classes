{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardmax(W,H):\n",
    "  d_dist = W @ H.T\n",
    "  wh = torch.diag(d_dist)\n",
    "  matrix = d_dist - wh.unsqueeze(1).repeat((1,W.shape[0]))\n",
    "  for i in range(W.shape[0]):\n",
    "      matrix[i, i] = -np.inf\n",
    "  max = torch.max(matrix)\n",
    "  return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize(W, H, alpha=0.1, tol=1e-6, max_iter=500000):\n",
    "    \"\"\"\n",
    "    Use gradient descent to minimize the objective function.\n",
    "    \"\"\"\n",
    "    lr_sched = np.linspace(0, alpha, num=max_iter)\n",
    "    lr_sched = lr_sched[::-1]\n",
    "    W = torch.autograd.Variable(W, requires_grad=True)\n",
    "    H = torch.autograd.Variable(H, requires_grad=True)\n",
    "    for i in range(max_iter):\n",
    "        f = hardmax(W, H)\n",
    "        # f = torch.nn.functional.cross_entropy(W@H.T*1, torch.arange(0, W.shape[0]).type(torch.LongTensor).to(W.device))\n",
    "        f.backward()\n",
    "        if torch.norm(W.grad) < tol and torch.norm(H.grad):\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            W -= lr_sched[i] * W.grad\n",
    "            W /= torch.norm(W, dim=1, keepdim=True)\n",
    "            W.grad.zero_()\n",
    "            H -= lr_sched[i] * H.grad\n",
    "            H /= torch.norm(H, dim=1, keepdim=True)\n",
    "            H.grad.zero_()\n",
    "        if i%5000 == 0:\n",
    "          print(\"iteration \" + str(i).zfill(7) +\" lr: %.3f\"%lr_sched[i]+\" f_value: %.8f\" %f.item() + \" max difference: %.5f\"%torch.max(torch.norm(W-H, dim=1)).item())\n",
    "    return f, W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d: 21, K: 162\n",
      "init_f:  tensor(-0.7563, device='cuda:3')\n",
      "iteration 0000000 lr: 0.100 f_value: -0.75628614 max difference: 0.00000\n",
      "iteration 0005000 lr: 0.097 f_value: -0.70484447 max difference: 0.00000\n",
      "iteration 0010000 lr: 0.095 f_value: -0.71426630 max difference: 0.00000\n",
      "iteration 0015000 lr: 0.092 f_value: -0.71601117 max difference: 0.00000\n",
      "iteration 0020000 lr: 0.090 f_value: -0.70593172 max difference: 0.00000\n",
      "iteration 0025000 lr: 0.087 f_value: -0.72524649 max difference: 0.00000\n",
      "iteration 0030000 lr: 0.085 f_value: -0.74361879 max difference: 0.00000\n",
      "iteration 0035000 lr: 0.082 f_value: -0.77657616 max difference: 0.00000\n",
      "iteration 0040000 lr: 0.080 f_value: -0.79640341 max difference: 0.00000\n",
      "iteration 0045000 lr: 0.077 f_value: -0.82122707 max difference: 0.00000\n",
      "iteration 0050000 lr: 0.075 f_value: -0.85032749 max difference: 0.00000\n",
      "iteration 0055000 lr: 0.072 f_value: -0.87039530 max difference: 0.00000\n",
      "iteration 0060000 lr: 0.070 f_value: -0.87776345 max difference: 0.00000\n",
      "iteration 0065000 lr: 0.067 f_value: -0.89084709 max difference: 0.00000\n",
      "iteration 0070000 lr: 0.065 f_value: -0.89805937 max difference: 0.00000\n",
      "iteration 0075000 lr: 0.062 f_value: -0.90497690 max difference: 0.00000\n",
      "iteration 0080000 lr: 0.060 f_value: -0.90651280 max difference: 0.00000\n",
      "iteration 0085000 lr: 0.057 f_value: -0.92021114 max difference: 0.00000\n",
      "iteration 0090000 lr: 0.055 f_value: -0.92600727 max difference: 0.00000\n",
      "iteration 0095000 lr: 0.052 f_value: -0.92131293 max difference: 0.00000\n",
      "iteration 0100000 lr: 0.050 f_value: -0.93113041 max difference: 0.00000\n",
      "iteration 0105000 lr: 0.047 f_value: -0.93723845 max difference: 0.00000\n",
      "iteration 0110000 lr: 0.045 f_value: -0.94392276 max difference: 0.00000\n",
      "iteration 0115000 lr: 0.042 f_value: -0.94711864 max difference: 0.00000\n",
      "iteration 0120000 lr: 0.040 f_value: -0.95052111 max difference: 0.00000\n",
      "iteration 0125000 lr: 0.037 f_value: -0.94602036 max difference: 0.00000\n",
      "iteration 0130000 lr: 0.035 f_value: -0.95577878 max difference: 0.00000\n",
      "iteration 0135000 lr: 0.032 f_value: -0.96015370 max difference: 0.00000\n",
      "iteration 0140000 lr: 0.030 f_value: -0.96515644 max difference: 0.00000\n",
      "iteration 0145000 lr: 0.027 f_value: -0.96318567 max difference: 0.00000\n",
      "iteration 0150000 lr: 0.025 f_value: -0.97097677 max difference: 0.00000\n",
      "iteration 0155000 lr: 0.022 f_value: -0.97432339 max difference: 0.00000\n",
      "iteration 0160000 lr: 0.020 f_value: -0.97678322 max difference: 0.00000\n",
      "iteration 0165000 lr: 0.017 f_value: -0.97625589 max difference: 0.00000\n",
      "iteration 0170000 lr: 0.015 f_value: -0.98189449 max difference: 0.00000\n",
      "iteration 0175000 lr: 0.012 f_value: -0.98258698 max difference: 0.00000\n",
      "iteration 0180000 lr: 0.010 f_value: -0.98465222 max difference: 0.00000\n",
      "iteration 0185000 lr: 0.007 f_value: -0.98833185 max difference: 0.00000\n",
      "iteration 0190000 lr: 0.005 f_value: -0.99041897 max difference: 0.00000\n",
      "iteration 0195000 lr: 0.002 f_value: -0.99236429 max difference: 0.00000\n",
      "max cosine value: 0.0055534383\n",
      "[0.0055534383]\n"
     ]
    }
   ],
   "source": [
    "#d_list = [3]#[3,4,8,7,6,5]\n",
    "#K_list = [12]#[12,120,240,56,27,16]\n",
    "#d_K_pair = [(7,56), (6,27), (5,16), (4, 120), (8,240)]\n",
    "d_K_pair = [(21,162)]\n",
    "cos_list = []\n",
    "lr = 0.1\n",
    "device = \"cuda:3\"\n",
    "for (d,K) in d_K_pair:\n",
    "    print(f\"d: {d}, K: {K}\")\n",
    "    W = torch.randn((K, d)).to(device)\n",
    "    #W_np = np.load(\"./WWT_matrix/d21_K162.npy\")\n",
    "    #W = torch.tensor(W_np).to(device)\n",
    "    W /= torch.norm(W, dim=1, keepdim=True)\n",
    "    H = W\n",
    "\n",
    "    init_f= hardmax(W, H)\n",
    "    print('init_f: ', init_f)\n",
    "    minimizer, W, H = minimize(W, H, alpha=lr)\n",
    "\n",
    "\n",
    "    WWT = (W @ W.T).detach().cpu().numpy()\n",
    "    with open(f'./WWT_matrix/d{d}_K{K}.npy', 'wb') as f:\n",
    "      np.save(f, WWT)\n",
    "\n",
    "    for i in range(WWT.shape[0]):\n",
    "        WWT[i,i] = -np.inf\n",
    "    print(\"max cosine value:\", np.max(WWT))\n",
    "    cos_list.append(np.max(WWT))\n",
    "\n",
    "print(cos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
